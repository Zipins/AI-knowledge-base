import os
import re
from typing import List, Tuple

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document
from load_kb import load_documents_from_drive

# æ¨¡å¼ï¼š
# runtime = äº‘ç«¯ï¼ˆåªè¯»å‘é‡åº“ + fallbackï¼Œä¸è®¿é—® Driveï¼‰
# refresh = æ¯æ—¥æ›´æ–°ï¼ˆè®¿é—® Drive + é‡å»ºå‘é‡åº“ï¼‰
KB_MODE = os.getenv("KB_MODE", "runtime")

PERSIST_DIR = "kb_chroma"


SYSTEM_PROMPT = """
ä½ æ˜¯å…¬å¸å†…éƒ¨çš„çŸ¥è¯†åº“åŠ©æ‰‹ã€‚
ä½ åªèƒ½æ ¹æ®â€œæä¾›ç»™ä½ çš„æ–‡æ¡£å†…å®¹â€æ¥å›ç­”é—®é¢˜ï¼Œç¦æ­¢æ ¹æ®å¤–éƒ¨çŸ¥è¯†æˆ–è‡†æµ‹å›ç­”ã€‚

å›ç­”è§„åˆ™ï¼š
1. å¦‚æœæœ‰æä¾›æ–‡æ¡£å†…å®¹ï¼š
   - å¿…é¡»åŸºäºæ–‡æ¡£å†…å®¹åšå‡ºåˆç†æ¨æ–­ï¼Œä¸å¾—è¯´â€œæ²¡æœ‰ç›¸å…³ä¿¡æ¯â€ã€‚
2. å¦‚æœå®Œå…¨æ²¡æœ‰æ–‡æ¡£å†…å®¹ï¼š
   - æ‰å¯ä»¥å›ç­”â€œå½“å‰çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ï¼Œè¯·å’¨è¯¢è´Ÿè´£äººâ€ã€‚
3. ç¦æ­¢å¼•ç”¨å¤–éƒ¨ç½‘ç»œçŸ¥è¯†ã€‚
"""


# ======================
#  å‘é‡åº“ç®¡ç†
# ======================

def rebuild_vectordb():
    """refresh æ¨¡å¼ï¼šä» Google Drive åŠ è½½æ–‡æ¡£å¹¶é‡å»ºå‘é‡åº“"""
    print(">>> [refresh] å¼€å§‹ä» Google Drive åŠ è½½æ–‡æ¡£...")
    docs = load_documents_from_drive()

    print(f">>> [refresh] åŠ è½½åˆ° {len(docs)} æ¡æ–‡æ¡£ï¼Œå¼€å§‹æ„å»ºå‘é‡åº“...")
    embeddings = OpenAIEmbeddings()

    vectordb = Chroma.from_documents(
        docs,
        embedding=embeddings,
        persist_directory=PERSIST_DIR,
    )
    vectordb.persist()
    print(">>> [refresh] å‘é‡åº“æ„å»ºå®Œæˆï¼")
    return vectordb


def get_vectordb() -> Chroma:
    """æ ¹æ®æ¨¡å¼åŠ è½½æˆ–æ„å»ºå‘é‡åº“"""

    if KB_MODE == "refresh":
        return rebuild_vectordb()

    # runtime æ¨¡å¼ â€”â€” åªåŠ è½½æœ¬åœ°å‘é‡åº“ï¼Œä¸è®¿é—® Drive
    embeddings = OpenAIEmbeddings()
    vectordb = Chroma(
        embedding_function=embeddings,
        persist_directory=PERSIST_DIR,
    )
    return vectordb


# ======================
#  æ–‡æœ¬æ ¼å¼åŒ–
# ======================

def format_docs(docs) -> str:
    chunks = []
    for i, d in enumerate(docs, start=1):
        source = (
            d.metadata.get("source")
            or d.metadata.get("name")
            or d.metadata.get("title")
            or "Unknown"
        )
        chunks.append(f"[æ–‡æ¡£ç‰‡æ®µ {i} - æ¥æºï¼š{source}]\n{d.page_content}\n")
    return "\n\n".join(chunks)


# ======================
#  å…³é”®å­—æå–
# ======================

def extract_keywords_from_question(question: str) -> List[str]:
    raw = re.findall(r"[A-Za-z0-9\-]+", question)
    out = []

    for tok in raw:
        tok = tok.strip("-").lower()
        if len(tok) < 2:
            continue
        if tok.isdigit() and len(tok) < 3:
            continue
        out.append(tok)

    uniq, seen = [], set()
    for k in out:
        if k not in seen:
            uniq.append(k)
            seen.add(k)
    return uniq


# ======================
#  ğŸ”¥å…³é”®å­—å…œåº•ï¼ˆruntime æ¨¡å¼ä¹Ÿå¯ç”¨ï¼‰
# ======================

def keyword_fallback_search(question: str, vectordb, max_hits: int = 5) -> List[Document]:
    """
    keyword fallbackï¼ˆå…œåº•æœç´¢ï¼‰ï¼š
    â€”â€” ä¸è®¿é—® Google Driveï¼ˆruntime æ¨¡å¼å®‰å…¨ï¼‰
    â€”â€” ç›´æ¥åœ¨å‘é‡åº“ä¸­çš„å…¨éƒ¨ chunks åšå…¨æ–‡æœç´¢
    """
    keywords = extract_keywords_from_question(question)
    if not keywords:
        return []

    # è·å–å‘é‡åº“ä¸­æ‰€æœ‰æ–‡æœ¬
    data = vectordb._collection.get(include=["documents", "metadatas"])
    docs = []

    for content, meta in zip(data["documents"], data["metadatas"]):
        text_lower = content.lower()

        if any(kw in text_lower for kw in keywords):
            docs.append(Document(page_content=content, metadata=meta))
            if len(docs) >= max_hits:
                break

    return docs


# ======================
#   ä¸»å›ç­”å‡½æ•°
# ======================

def answer_question(question: str, k: int = 8) -> Tuple[str, List[Document]]:
    vectordb = get_vectordb()

    # 1) è¯­ä¹‰æ£€ç´¢
    docs = vectordb.similarity_search(question, k=k)

    # 2) å…³é”®å­—å…œåº• â€”â€” runtime æ¨¡å¼ä¹Ÿå¯ç”¨ï¼ï¼
    keywords = extract_keywords_from_question(question)
    if keywords:
        joined = "\n".join(d.page_content.lower() for d in docs)
        missing = [kw for kw in keywords if kw not in joined]

        if missing:
            extra_docs = keyword_fallback_search(question, vectordb)
            # é¿å…é‡å¤
            existing = set(
                (hash(d.page_content), d.metadata.get("source")) for d in docs
            )
            for d in extra_docs:
                key = (hash(d.page_content), d.metadata.get("source"))
                if key not in existing:
                    docs.append(d)
                    existing.add(key)

    # 3) æ„é€  context
    context = format_docs(docs)

    # 4) è®©æ¨¡å‹å›ç­”
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1)

    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {
            "role": "user",
            "content": (
                f"ç”¨æˆ·é—®é¢˜ï¼š{question}\n\n"
                f"ä»¥ä¸‹æ˜¯å’Œé—®é¢˜æœ€ç›¸å…³çš„çŸ¥è¯†åº“å†…å®¹ï¼š\n{context}\n\n"
                f"è¯·ä¸¥æ ¼æ ¹æ®ä»¥ä¸Šå†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"
            ),
        },
    ]

    resp = llm.invoke(messages)
    return resp.content, docs


# ======================
#   CLI æ¨¡å¼
# ======================

if __name__ == "__main__":
    print(f"å½“å‰æ¨¡å¼ï¼šKB_MODE={KB_MODE}")
    while True:
        q = input("\nè¯·è¾“å…¥é—®é¢˜ï¼ˆexit é€€å‡ºï¼‰ï¼š")
        if q.lower() in ["exit", "quit"]:
            break
        ans, ds = answer_question(q)
        print("\nå›ç­”ï¼š", ans)
        print("å¼•ç”¨ç‰‡æ®µæ•°é‡ï¼š", len(ds))
